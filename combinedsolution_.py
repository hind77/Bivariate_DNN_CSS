# -*- coding: utf-8 -*-
"""combinedSolution .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d1SHmb0Gnf5LrQRyEYXrVA8N9TU6FIAb

#Global Vars
"""
import tensorflow as tf
import numpy as np
from scipy import special as sp
import math
tf.config.run_functions_eagerly(True)
print(tf.__version__)

size_area=200.0 # size of area sensors are distributed 

pu_active_prob = 0.7

pl_const = 34.5 # use pathloss constant   (wslee paper)
pl_alpha = 38.0 # use pathloss constant   (wslee paper)

d_ref = 50.0 # reference distance 
sh_sigma = 7.9 # shadow fading constant   (wslee paper)

p_t_dB = 23.0 # tx power - 23dBm
p_t = 10*(p_t_dB/10)

sigma_v = 1 # noice variance

alpha = 0.0001
batch_s = 512
batch_id = batch_s
num_sens = 10 
samples_factor = 10000
num_samples = batch_s*samples_factor

sen_loc = size_area*(np.random.rand(num_sens, 2)-0.5)
pri_loc = size_area*(np.random.rand(1, 2)-0.5) #placing sensing entities and primary user randomly

pf = 0.01
pd = np.arange(0, 1, 0.05)# probability of detection
val = 1-2*pf
thresh = ((math.sqrt(2)*sp.erfinv(val))/ math.sqrt(num_samples))+1

rounds = 10

n=50

local_pds = {k: [] for k in range(num_sens)}
cooperative_pds = list()

pi_0 = 0.3
pi_1 = 0.7
fs = 100*1000
tr = 0.2*pow(10,-3)
T_cte = (n/fs)+num_sens*tr
pf_threshold = 0.01
pd_threshold = 0.5
snrs_count = -1
counter = 1
counter_pe = 0

Pe_values = []
final_Pe_values = []

"""# Channel Model"""

import numpy as np
 
 
class ChannelModel:
    
    
    @staticmethod
    def get_distances() -> np.ndarray:
    
        '''
        this function generate the random distribution of secondary users and the primary user
        
        '''    
        
        dist_pr_su_vec = pri_loc.reshape(1, 2) - sen_loc # Generate PU-SU distance_vector
        dist_pr_su_vec = np.maximum(dist_pr_su_vec, 0.1)
        dist_pr_su_vec = np.linalg.norm(dist_pr_su_vec, axis=1)
        dist_su_su_vec = sen_loc.reshape(num_sens, 1, 2) - sen_loc # Generate SU-SU distance_vector
        dist_su_su_vec = np.linalg.norm(dist_su_su_vec, axis=2)
        
        return dist_pr_su_vec, dist_su_su_vec
    
    
    @staticmethod
    def get_channel_gain(dist_pr_su_vec: np.ndarray) -> np.ndarray:
    
        '''
        this function generates the channel gain of each secondary user using the distance
        
        Parameters:
         dist_pr_su_vec : distance between secondary users and the primary user
    
        Output:
            channel gain
        
        '''  
        
        pu_ch_gain_db = - pl_const - pl_alpha * np.log10(dist_pr_su_vec) # primary channel gain
        return 10 ** (pu_ch_gain_db / 10)
    
    
    @staticmethod
    def get_secondary_correlation(dist_su_su_vec: np.ndarray ) -> np.ndarray:
        '''
        this function computes the secondary users correlation using SU-SU distances
        
        Parameters:
         dist_su_su_vec : distances between the secondary users
    
        Output:
            secondary users correlation
        '''    
        return np.exp(-dist_su_su_vec / d_ref)
 
 
    @staticmethod
    def get_shadowing(su_cor: np.ndarray, num_sens: int) -> np.ndarray:
            '''
            this function computes the shadowing using SU-SU correlation 
            
            Parameters:
             num_sens : number of sensing units
             su_cor : the correlation between the secondary users
        
            Output:
                shadowing    
            
            ''' 
            shadowing_dB = sh_sigma * np.random.multivariate_normal(np.zeros([num_sens]), su_cor)
            return 10 ** (shadowing_dB / 10)
 
 
    @staticmethod
    def get_multiPath_Fading(num_sens: int) -> np.ndarray:
        '''
        this function computes the multipath fading 
        
        Parameters:
         num_sens : number of sensing units
    
        Output:
            multipath fading 
        '''     
        multi_fading = 0.5 * np.random.randn(num_sens) ** 2 + 0.5 * np.random.randn(num_sens) ** 2
        return multi_fading ** 0.5
 
    
    @classmethod
    def ch_gen(cls, num_samples: int) -> np.ndarray:
      """ 
      This function deploy the channel model.
       Parameters:
            num_samples : number of samples
    
        Output:
           signal noise ratio
      
      """
    
      returned_power = []
      returned_SNRs = []
      returned_gain = []
      
      
      for i in range(num_samples):
    
        dist_pr_su_vec, dist_su_su_vec = cls.get_distances()
        pu_ch_gain =  cls.get_channel_gain(dist_pr_su_vec)
        su_cor =  cls.get_secondary_correlation(dist_su_su_vec)
        shadowing =  cls.get_shadowing(su_cor,num_sens)
        pu_power = np.zeros([len(su_cor)]) #pu_power (received power initialization)
        noise = np.random.randn(num_sens)
        #SNR = np.zeros([len(su_cor)])
        pri_power = p_t #pri_power (transmitted power)
        pu_ch_gain_tot = pu_ch_gain 
        # test the activity of the primary user 
        if (np.random.rand() < pu_active_prob):
          pu_ch_gain_tot = pu_ch_gain  * shadowing + noise
          pu_power = pu_power +  pri_power*pu_ch_gain_tot 
          SNR = pri_power * pow(abs(pu_ch_gain_tot),2)/ sigma_v
        else:
          SNR = noise
        multi_fading =  cls.get_multiPath_Fading(num_sens)
        pu_power = pu_power * multi_fading
        returned_power.append(pu_power)
        returned_SNRs.append(SNR)
        returned_gain.append(pu_ch_gain_tot)
        
        
      output = dict()
      output['snrs'] = returned_SNRs
      output['gain'] = returned_gain
      
    
      return output

"""# Global DNN Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization
 
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow.keras import regularizers
from statistics import mean 
import matplotlib.pyplot as plt
 
class DNNModel:
    

    def get_model():
        
        """ 
        This function returns sequential model and the model initilizer
      
        """
        model = Sequential()
        model.add(BatchNormalization(input_shape=(num_sens,)))
        initializer = tf.keras.initializers.GlorotUniform()

        return model, initializer     
 
 

    def train_model(model, X_train: np.ndarray, X_val: np.ndarray,
              y_train: np.ndarray,
              y_val: np.ndarray, loss_fn) -> np.ndarray:
        """
        This function trains the model. The number of epochs and 
        batch_size are set by the constants in the parameters section.
        
        Parameters:
            model : model with the chosen architecture
            X_train : training features
            y_train : training target
            X_valid : validation features
            Y_valid : validation target
            
        Output:
            model training history    
        
        """
        history = []
        opt = tf.keras.optimizers.Adam(learning_rate=1e-4)
        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50) 
        model.compile(loss = loss_fn , optimizer=opt, metrics=['acc'])
        history = model.fit(X_train,y_train,batch_size=batch_s,epochs=40,
                            validation_data=(X_val,y_val),
                            callbacks=[callback])
        print("this is the pe values",Pe_values)
        # stock Pe
        with open('Pe.data', 'wb') as filehandle: 
            pickle.dump(Pe_values, filehandle)
            print("pe is stocked")
        # plt.figure(3)
        # ax = plt.axes()
        # ax.plot(Pe_values, linestyle= 'dotted', color='blue',label='Pe')
        # ax.set_title('Probability of error ')
        # ax.margins(x=0,y=0)
        # ax.grid(False)
        # plt.legend()
        # ax.set_xlabel('Epoch number')
        # ax.set_ylabel('Pe')
        # #plt.show()  
        # plt.savefig('Pe metric in.pdf')
        # plt.close()
        return history
    
  

    def eval_metric(model, history: np.ndarray,
                    metric_name: str,problem_id: str) -> np.ndarray:
        '''
        Function to evaluate a trained model on a chosen metric. 
        Training and validation metric are plotted in a
        line chart for each epoch.
        
        Parameters:
            history : model training history
            metric_name : loss or accuracy
        Output:
            line chart with epochs of x-axis and metric on
            y-axis
        '''   
        for key in history.history.keys():
            print(key)
        metric = history.history[metric_name]
        val_metric = history.history['val_' + metric_name] 
        plt.figure(2)
        ax = plt.axes()
        ax.plot(metric, linestyle= 'dashdot', marker='+', color='blue',label='Train Loss')
        ax.plot(val_metric, linestyle='dashdot', marker='+', color='red',label='Val Loss')
        ax.set_title('Loss ' + model.name)
        ax.margins(x=0,y=0)
        ax.grid(False)
        plt.legend()
        ax.set_xlabel('Epoch number')
        ax.set_ylabel(metric_name)
        #plt.show()  
        plt.savefig(metric_name+' metric in '+model.name +' for '+problem_id +'.pdf')
        plt.close()
 
  
    def compare_models(model_1, model_2, 
                       
                       history_1: np.ndarray,
                       history_2: np.ndarray, 
                        
                      
                       metric: str, problem_id: str):
    
        '''
        Function to compare a metric between two models 
        
        Parameters:
            history_1 : training history of model 1
            history_2 : training history of model 2
            metric : metric to compare, loss, acc, val_loss or val_acc
            
        Output:
            plot of metrics of both models
        '''    
        metric_1 = history_1.history[metric]
        metric_2 = history_2.history[metric]
        #metric_3 = history_3.history[metric]
        #metric_4 = history_4.history[metric]
        
        metrics_dict = {
            'acc' : 'Training Accuracy',
            'loss' : 'Training Loss',
            'val_acc' : 'Validation accuracy',
            'val_loss' : 'Validation loss'
        } 
        
        metric_label = metrics_dict[metric]
    
        plt.figure(3)
        ax = plt.axes()    
        ax.plot(metric_1, linestyle= 'solid', color='orange', label=model_1.name)
        ax.plot(metric_2, linestyle= 'solid', color='green', label=model_2.name)
        #ax.plot(metric_3, linestyle= 'solid', color='blue', label = model_3.name)
        #ax.plot(metric_4, linestyle= 'solid', color='black', label = model_4.name)
        ax.margins(x=0,y=0)
        plt.xlabel('Epoch number')
        plt.ylabel(metric_label)
        plt.title('Comparing ' + metric_label + ' between models for '+ problem_id)
        plt.legend()
        #plt.show()
        plt.savefig('Comparing ' + metric_label + ' between models'+ ' for the problem '+ problem_id +'.pdf')
        plt.close()

"""# DNN Model 3 Vars"""

import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras import backend as K
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow.keras import regularizers
from numpy import exp
# Custom activation function
#from tensorflow.keras.layers import Activation
#from tensorflow.keras.utils.generic_utils import get_custom_objects
 
 
 
    
 
class BivariateDNNModel(DNNModel):
    @classmethod
    def bivariate_loss_gain(cls,gain):
        global batch_id
        global snrs_count
        global Pe_values
        global final_Pe_values
        
        dim=batch_s * num_sens 
        gain= gain[0:batch_id,:]
        gain= tf.cast(gain, tf.float32)
        gain_bis = tf.reshape(gain,(batch_s,num_sens))
        snrs_count = snrs_count + 1
        
        
        def bivariate_loss(snrs: np.ndarray, predicted) -> float:
          global counter
          global counter_pe
                    
          # Threshold Predicted value 
          predicted_threshold = predicted[:,0]
          
          # weights Predicted values 
          predicted_weights = predicted[:,1:]
          count = 0
          
          loss_values = []
          pe_values = []
          while count < batch_s:              
              snrs_s = snrs[count,:]
              C = tf.linalg.tensor_diag(1+2*snrs_s)
              val = predicted_threshold[count]
              diag_exp1 = tf.linalg.diag_part(tf.linalg.matmul(predicted_weights,tf.ones([10,batch_s])))
              test = fs*(T_cte - 10* tr)*diag_exp1
              exp1 = predicted_threshold[count]-test
              diag_exp2=  tf.linalg.diag_part(tf.linalg.matmul(predicted_weights,tf.keras.backend.transpose(predicted_weights)))
              exp2 = tf.math.sqrt(2*fs*(T_cte - 10* tr)* diag_exp2)
              p_0 = cls.Q(exp1/exp2)
              diag_exp3 = tf.linalg.diag_part(tf.linalg.matmul(predicted_weights,(0.3*tf.keras.backend.transpose(gain_bis)+tf.ones([10,batch_s]))))
              exp3 = predicted_threshold[count]- (fs*(T_cte - 10* tr))* diag_exp3
              diag_exp4 = tf.linalg.diag_part(tf.linalg.matmul(tf.linalg.matmul(predicted_weights,C),tf.keras.backend.transpose(predicted_weights)))
              exp4 = tf.math.sqrt(2*fs*(T_cte - 10* tr)*diag_exp4)
              p_1 = (1-cls.Q(exp3/exp4))
              Pf = cls.Q(exp1/exp2)
              Pf = tf.reduce_mean(tf.reduce_mean(Pf)) 
              if not Pf< pf_threshold:
                  Pf_penality = 0.0
              else:
                  Pf_penality = tf.math.pow((Pf-pf_threshold),2) 
              Pd = cls.Q(exp3/exp4) 
              Pd = tf.reduce_mean(tf.reduce_mean(Pd))
              if not Pd> pd_threshold:
                  Pd_penality = 0.0
              else:
                  Pd_penality = tf.math.pow((Pd-pd_threshold),2) 
              weight_penality = tf.math.pow((tf.norm(predicted_weights)-1),2) 
              Pe = pi_0*Pf+ pi_1*(1 - Pd)
              loss = Pe + alpha*(Pd_penality + Pf_penality + weight_penality)
              loss = tf.reduce_mean(tf.reduce_mean(loss))# remove 
              loss_values.append(loss)
              pe_values.append(Pe.numpy())
              count = count+1
          
          if counter == 7000:  
              counter = 0
              Pe_values.append(tf.reduce_mean(pe_values).numpy())
          counter = counter + 1       
          return tf.reduce_mean(loss_values)

        return bivariate_loss
  
    @staticmethod  
    def Q(x):
        """
        the Q-function
        """
        value = tf.cast(x, tf.float32)
        return 0.5-0.5*tf.math.erf(tf.cast(value/ math.sqrt(2),tf.float32))
    
    # calculate the softmax of a vector
    @staticmethod  
    def softmax(vector):
        	e = exp(vector)
        	return e / e.sum()
        
    @classmethod
    def custom_activation(cls,x):

        #tf.print("this is thethresh", x[:,0])
        #tf.print("this is the weights", x[:,1:])        
        #threshold = x[:,0]
        #tf.print("this is the thersold", threshold)
        #weights = x[:,1:]
        #tf.cast(weights, tf.float32)
        #tf.cast(threshold, tf.float32)
        #weights = tf.nn.softmax(x[:,1:])
        #threshold = tf.nn.relu(x[:,0])
        #tf.print("this is the thersold", weights)
        #tf.concat(threshold,weights)
        return x
        
    @classmethod
    def choose_model(cls,choice: str):
        """ 
        This function defines the model architecture and returns the model
      
        """  
        #get_custom_objects().update({'custom_activation': Activation(cls.custom_activation)})
        model = Sequential()
        model.add(BatchNormalization(input_shape=(num_sens,)))
        initializer = tf.keras.initializers.GlorotUniform()
        if choice == "dropout":            
                model.add(Dense(num_sens+1, kernel_initializer=initializer, kernel_regularizer=regularizers.l1(0.003)))
                model.add(LeakyReLU(alpha=0.05))
                model.add(layers.Dropout(0.4))
                model.add(BatchNormalization())
                model.add(Dense(6, kernel_initializer=initializer, kernel_regularizer=regularizers.l1(0.003)))
                model.add(LeakyReLU(alpha=0.005))
                model.add(layers.Dropout(0.4))
                model.add(BatchNormalization())
                model.add(Dense(6, kernel_initializer=initializer, kernel_regularizer=regularizers.l1(0.003)))
                model.add(LeakyReLU(alpha=0.005))
                model.add(layers.Dropout(0.4))
                model.add(BatchNormalization())
                model.add(Dense(6, kernel_initializer=initializer, kernel_regularizer=regularizers.l1(0.003)))
                model.add(LeakyReLU(alpha=0.005))
                model.add(layers.Dropout(0.4))
                model.add(BatchNormalization())
                model.add(Dense(num_sens+1, name='last_layer', kernel_initializer=initializer, kernel_regularizer=regularizers.l1(0.003)))
                #model.add(Activation(cls.custom_activation, name='SpecialActivation'))
                #model.add(LeakyReLU(alpha=0.005))
                model._name = 'dropout'                
        return model
 
 
        if choice == "dropout1":            
                model.add(Dense(num_sens, kernel_initializer=initializer))
                model.add(LeakyReLU(alpha=0.05))
                model.add(layers.Dropout(0.3))
                model.add(BatchNormalization())
                model.add(Dense(64, kernel_initializer=initializer))
                model.add(LeakyReLU(alpha=0.005))
                model.add(layers.Dropout(0.3))
                model.add(BatchNormalization())
                model.add(Dense(32, kernel_initializer=initializer))
                model.add(LeakyReLU(alpha=0.005))
                model.add(layers.Dropout(0.3))
                model.add(BatchNormalization())
                model.add(Dense(16, kernel_initializer=initializer))
                model.add(LeakyReLU(alpha=0.005))
                model.add(layers.Dropout(0.3))
                model.add(BatchNormalization())
                model.add(Dense(1, name='last_layer', kernel_initializer=initializer,activation='softplus'))
                #model.add(LeakyReLU(alpha=0.005))
                model._name = 'dropout'                
        return model
    
"""# Mathematical Model"""

import numpy as np
import math 
from scipy.optimize import minimize, rosen, rosen_der
from numpy import linalg as LA
import itertools
from numpy.linalg import multi_dot
from scipy import special
import random

class MathematicalModel:
        
    @classmethod
    def new_paper_mathematical_weights(cls,snrs: np.ndarray)-> np.ndarray:
    
        '''
        Function to compute the weights from the mathematical formula in 2016 paper
        
        Parameters:
            
            snrs:  the row of  the sensing units snrs 
            signak_power: the power of the signal received by the sensing units
            
        Output:
            mathematical weights
        '''      
        C = np.diag(1+2*snrs)
        C_inv = np.linalg.pinv(C)
        norm = np.linalg.norm(np.dot(C_inv,snrs), ord=2)  
        w_opt = np.dot( C_inv, snrs)/norm
        w_opt = cls.real(w_opt)
        w_opt = np.array(w_opt)   
        return w_opt
    
    
    @staticmethod
    def compute_deflection_coef(weights: np.ndarray, snrs: np.ndarray)-> float:
        '''
        Function to compute the deflection coef from the mathematical 
        formula in 2007 paper
        
        Parameters:
            
            snrs: the row of  the sensing units snrs 
            weights: weights array 
            
        Output:
            mathematical weights
        '''        
                
        
        snrs = snrs.transpose()
        snrs = snrs.reshape(10,1)
        weights = weights.reshape(10,1)
        t = np.dot(snrs.transpose(),weights)**2
        t1 = np.dot(4*weights.transpose(),
                    (n*np.identity(num_sens)+np.diag(snrs)))
        b = np.dot(t1,weights)       
        dm_square = t/b
        
        return dm_square
    
    
    @staticmethod
    def compute_weights_using_deflection_coef(snrs) -> np.ndarray:
        '''
        This function is the numerical computation of the weights using the minimization function
        
         Parameters:
             snrs: secondary users snrs
         output:
             the optimal weights
        
        '''
        #fun = lambda x: ((x[i]*nu[i] for i in range(0,len(nu)))**2)/(4*((n+nu[i])*x[i]**2 for i in range(0,len(nu))))
        n = 50
        nu = snrs
        fun = lambda x: - ((x[0]*nu[0] + x[1]*nu[1]+x[2]*nu[2]+x[3]*nu[3]+x[4]*nu[4]+x[5]*nu[5]+x[6]*nu[6]+x[7]*nu[7]+x[8]*nu[8]+x[9]*nu[9])**2)/(4*((n+nu[0])*x[0]**2+(n+nu[1])*x[1]**2+(n+nu[2])*x[2]**2+(n+nu[3])*x[3]**2+(n+nu[4])*x[4]**2+(n+nu[5])*x[5]**2+(n+nu[6])*x[6]**2+(n+nu[7])*x[7]**2+(n+nu[8])*x[8]**2+(n+nu[9])*x[9]**2))       
        #fun = lambda x: (sum_s(nu, x))/mul(nu, x)        
        constraint = ({'type': 'eq', 'fun': lambda x:  LA.norm(x,1)-1})
        bnds = ((0, 1),) * len(snrs)
        random_guess = np.random.dirichlet(np.ones(10)*1000.,size=1)
        random_guess = tuple(list(random_guess[0]))
        res = minimize(fun, random_guess, method='SLSQP', bounds=bnds, constraints=constraint)
        return(res.x)
    
    
    @staticmethod
    def Q(x):
        """
        the Q-function
        """
        
        return 0.5-0.5*special.erf(x/ math.sqrt(2))
    
    
    @classmethod
    def func(cls,x, *args):
    
        """
        The optimization function from the 2016 paper
        
        """
        snrs=np.array(args[0])

        epsilon = 0.05 # to avoid the division by 0 
        h = np.ones(num_sens)# I set h for ones to simplify the computation
        h = h.reshape(10,1)# I reshape the snrs for matricial multiplication 
        bold_one = np.identity(num_sens)
        C = np.diag(1+2*snrs) # the C value from the formula diag{[1 +2snrs]}
        C_inv = np.linalg.inv(C) # C_inv
        snrs_s = snrs.reshape(10,1) 
        exp1 = multi_dot([snrs_s.transpose(), C_inv.transpose(),h])
        exp2 = np.linalg.norm(np.dot( C_inv,snrs_s), ord=1)
        identity_vector = np.ones(num_sens)
        identity_vector = identity_vector.reshape(10,1)
        add = identity_vector+h       
        exp3 = num_sens* multi_dot([snrs_s.transpose(), C_inv.transpose(),add])- (x*np.linalg.norm(np.dot( C_inv,snrs_s), ord=1))
        exp4 = np.sqrt(multi_dot([snrs_s.transpose(), C_inv.transpose(), snrs_s])+epsilon)
        t1 = np.divide(exp1, exp2)
        t2 = np.divide(exp3, exp4)
        p_0 = pi_0*cls.Q(x-(exp1/exp2))
        p_1 = pi_1*cls.Q(exp3/exp4)
        f = p_0 + p_1
        print("this is the mathematical pe",f[0][0])
        return f[0][0]
    
    
    @classmethod
    def compute_numerical_thresholds(cls,snrs):
        
        '''
        This function compute the numerical optimal thresholds using minimization
        
          Parameters:
              snrs of the secondary users
          Outputs:
              numerical optimal thresholds
        '''
        bnds = ((0, 1),)  
        random_guess = random.choice(snrs)
        constraint = ({'type': 'ineq', 'fun': lambda x:  x>0})
        res = minimize(cls.func, random_guess,args=(snrs), method='SLSQP', bounds=bnds, constraints=constraint)
        return res.x 
     
    
    @staticmethod
    def real(w1):
        return w1.real
    

import math 
from scipy import special as sp
import scipy.spatial.distance
import matplotlib.pyplot as plt
from texttable import Texttable
from memory_profiler import profile

class SpectrumSensing:
    
    @profile         
    def generate_thresholds(cls,snrs_test) -> np.ndarray :
        #thresholds_d = model_d.predict(snrs_test)
        #dnn_thresholds_d = thresholds_d[0]
        for i in range(0,snrs_test.shape[0]):
            numerical_thresholds = MathematicalModel.compute_numerical_thresholds(snrs_test[i])
        #print("DNN thresholds for dropeout model ",dnn_thresholds_d)
        #print("numerical thresholds",numerical_thresholds)   
        
"""# Main"""

from sklearn import preprocessing 
from sklearn.model_selection import train_test_split 
import pickle

def generate_data(choice):
    
     if choice == '1':
         with open('trainData.data', 'rb') as filehandle:
             output_train = pickle.load(filehandle) 
             
     if choice == '2':
         output_train =ChannelModel.ch_gen(num_samples)
         
     return output_train
         

def main():
    
    print("Please define your data generation method\n")
    print("press 1 for stocked samples\n")
    print("press 2 for new samples\n")
    choice = input("press your choice: ")
    output_train = generate_data(choice)
    output_test =ChannelModel.ch_gen(num_samples)
    snrs_train = np.array(output_train['snrs'])
    # stockData
    # with open('trainData.data', 'wb') as filehandle: 
    #       pickle.dump(output_train, filehandle)
    snrs_test = np.array(output_train['snrs'])
    signal_gain= np.array(output_train['gain'])
    print("snrs plot", snrs_train)
    spectrum_sensing = SpectrumSensing()
    
    #train data scaling
    scaler_train = preprocessing.StandardScaler().fit(snrs_train)
    snrs_train = scaler_train.transform(snrs_train)
    snrs_train = np.absolute(snrs_train)

    #test data scaling 
    scaler_test = preprocessing.StandardScaler().fit(snrs_test)
    snrs_test = scaler_test.transform(snrs_test)
    snrs_test = np.absolute(snrs_test)
    
        


    X_train, X_val, y_train, y_val = train_test_split(snrs_train, snrs_train, test_size=0.30)
    
    #the model for the first problem(p1) that provided weights to maximize the deflection coef
    problem1_id = 'bivariate_problem'
    

    model = BivariateDNNModel.choose_model("dropout")
    

    history = BivariateDNNModel.train_model(model, X_train, X_val, y_train, y_val, BivariateDNNModel.bivariate_loss_gain(gain=signal_gain)) 
    

    BivariateDNNModel.eval_metric(model, history, "loss", problem1_id)
    #spectrum_sensing.generate_thresholds(snrs_test)
    
    
  
if __name__ == '__main__': 
    main()

